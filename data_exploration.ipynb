{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immigration Data Lake\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:3.0.0-s_2.12\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "fname = 'data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv in data folder\n",
    "df.to_csv('data/i94_apr16_sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a sample of the csv to the data folder\n",
    "df.sample(1000).to_csv('data/i94_apr16_sub_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:3.0.0-s_2.12\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_spark.write.parquet(\"data/sas_data\")\n",
    "df_spark=spark.read.parquet(\"data/sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read immigration data\n",
    "imm_df = spark.read.format('com.github.saurfang.sas.spark').load('data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# immigration data schema\n",
    "imm_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | I94RES | 3 digit code for country of residence |\n",
    "imm_df.groupBy('I94RES').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | I94ADDR | state of arrival |\n",
    "imm_df.groupBy('I94ADDR').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | I94VISA | visa codes collapsed into three categories: 1 = Business; 2 = Pleasure; 3 = Student |\n",
    "imm_df.groupBy('I94VISA').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_df.groupBy('I94PORT').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_df.select('I94VISA').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_df.groupBy('I94YR').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = imm_df.groupBy(\"INSNUM\").agg(countDistinct(\"GENDER\"))\n",
    "gr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.groupBy('count(GENDER)').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong date times, because its a SAS numeric time field\n",
    "from pyspark.sql.functions import col , column\n",
    "imm_df.withColumn(\"ts\", col(\"ARRDATE\").cast(\"timestamp\")).select('ts').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = spark.read.options(header='true', inferSchema='true').csv('data/GlobalLandTemperaturesByState.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.groupBy('State').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo\n",
    "demo_df = spark.read.options(header='true', inferSchema='true', delimiter=';').csv('data/us-cities-demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# airport\n",
    "airport_df = spark.read.options(header='true', inferSchema='true').csv('data/airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_df.groupBy('iso_country').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_df.filter((airport_df[\"ident\"] == \"\") | airport_df[\"ident\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_df.groupBy('ident').count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename cicid to id\n",
    "# rename i94yr to year\n",
    "# rename i94mon to month\n",
    "# read mapping for code to country of origin, and map the I94CIT var to the actual country of origin (do later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_fact = imm_df.select(col(\"cicid\").alias(\"id\"),\n",
    "                          col(\"i94yr\").alias(\"year\"),\n",
    "                          col(\"i94mon\").alias(\"month\"),\n",
    "                          col(\"I94CIT\").alias(\"country_of_origin\"),\n",
    "                          col(\"I94ADDR\").alias(\"state_of_arrival\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_fact.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_dim = imm_df.select(col(\"cicid\").alias(\"id\"),\n",
    "                          col(\"GENDER\").alias(\"gender\"),\n",
    "                          col(\"I94BIR\").alias(\"I94BIR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = demo_df.withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_dim = demo_df.select(col(\"id\"),\n",
    "                                  col(\"State\").alias(\"state\"),\n",
    "                                  col(\"MEDIAN AGE\").alias(\"median_age\"),\n",
    "                                  col(\"Total Population\").alias(\"total_population\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_dim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_dim = airport_df.select(col(\"ident\").alias(\"id\"),\n",
    "                                  col(\"iso_country\").alias(\"country\"),\n",
    "                                  col(\"MEDIAN AGE\").alias(\"median_age\"),\n",
    "                                  col(\"Total Population\").alias(\"total_population\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create country code mapping table using the I94 description file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"data/i94_country_code_mapping.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(np.nan, '', regex=True)\n",
    "cols = [1,2,3,4,5,6,7,8,9,10]\n",
    "df['country'] = df[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "df = df[[0, 'country']]\n",
    "df.columns = ['code', 'country']\n",
    "df = df[df['code'] != '']\n",
    "df['code'] = df['code'].astype(int)\n",
    "df.to_csv('data/code_to_country_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, create_map, lit, expr, to_date\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:3.0.0-s_2.12\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all tables\n",
    "immigration_staging = spark.read.options(header='true', inferSchema='true').csv('data/i94_apr16_sub_sample.csv')\n",
    "country_code_mapping_staging = spark.read.options(header='true', inferSchema='true').csv('data/code_to_country_mapping.csv')\n",
    "demographics_staging = spark.read.options(header='true', inferSchema='true', delimiter=';').csv('data/us-cities-demographics.csv')\n",
    "weather_staging = spark.read.options(header='true', inferSchema='true').csv('data/GlobalLandTemperaturesByState.csv')\n",
    "airport_staging = spark.read.options(header='true', inferSchema='true').csv('data/airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transportation_mapping = {1: 'air', 2: 'sea', 3: 'land', 4: 'not reported'}\n",
    "transportation_mapping_expr = create_map([lit(x) for x in chain(*transportation_mapping.items())])\n",
    "\n",
    "state_mapping = {'AL':'ALABAMA',\n",
    "                    'AK':'ALASKA',\n",
    "                    'AZ':'ARIZONA',\n",
    "                    'AR':'ARKANSAS',\n",
    "                    'CA':'CALIFORNIA',\n",
    "                    'CO':'COLORADO',\n",
    "                    'CT':'CONNECTICUT',\n",
    "                    'DE':'DELAWARE',\n",
    "                    'DC':'DIST. OF COLUMBIA',\n",
    "                    'FL':'FLORIDA',\n",
    "                    'GA':'GEORGIA',\n",
    "                    'GU':'GUAM',\n",
    "                    'HI':'HAWAII',\n",
    "                    'ID':'IDAHO',\n",
    "                    'IL':'ILLINOIS',\n",
    "                    'IN':'INDIANA',\n",
    "                    'IA':'IOWA',\n",
    "                    'KS':'KANSAS',\n",
    "                    'KY':'KENTUCKY',\n",
    "                    'LA':'LOUISIANA',\n",
    "                    'ME':'MAINE',\n",
    "                    'MD':'MARYLAND',\n",
    "                    'MA':'MASSACHUSETTS',\n",
    "                    'MI':'MICHIGAN',\n",
    "                    'MN':'MINNESOTA',\n",
    "                    'MS':'MISSISSIPPI',\n",
    "                    'MO':'MISSOURI',\n",
    "                    'MT':'MONTANA',\n",
    "                    'NC':'N. CAROLINA',\n",
    "                    'ND':'N. DAKOTA',\n",
    "                    'NE':'NEBRASKA',\n",
    "                    'NV':'NEVADA',\n",
    "                    'NH':'NEW HAMPSHIRE',\n",
    "                    'NJ':'NEW JERSEY',\n",
    "                    'NM':'NEW MEXICO',\n",
    "                    'NY':'NEW YORK',\n",
    "                    'OH':'OHIO',\n",
    "                    'OK':'OKLAHOMA',\n",
    "                    'OR':'OREGON',\n",
    "                    'PA':'PENNSYLVANIA',\n",
    "                    'PR':'PUERTO RICO',\n",
    "                    'RI':'RHODE ISLAND',\n",
    "                    'SC':'S. CAROLINA',\n",
    "                    'SD':'S. DAKOTA',\n",
    "                    'TN':'TENNESSEE',\n",
    "                    'TX':'TEXAS',\n",
    "                    'UT':'UTAH',\n",
    "                    'VT':'VERMONT',\n",
    "                    'VI':'VIRGIN ISLANDS',\n",
    "                    'VA':'VIRGINIA',\n",
    "                    'WV':'W. VIRGINIA',\n",
    "                    'WA':'WASHINGTON',\n",
    "                    'WI':'WISCONSON',\n",
    "                    'WY':'WYOMING' ,\n",
    "                    '99':'All Other Codes'}\n",
    "state_mapping = dict((k, v.title()) for k, v in state_mapping.items())\n",
    "state_mapping_expr = create_map([lit(x) for x in chain(*state_mapping.items())])\n",
    "\n",
    "visa_mapping = {1: 'business', 2: 'pleasure', 3: 'student'}\n",
    "visa_mapping_expr = create_map([lit(x) for x in chain(*visa_mapping.items())])\n",
    "\n",
    "immigration_fact =  immigration_staging.select(col(\"cicid\").alias(\"id\").cast(\"int\"),\n",
    "                    col(\"I94YR\").alias(\"year\").cast(\"int\"),\n",
    "                    col(\"I94MON\").alias(\"month\").cast(\"int\"),\n",
    "                    col(\"I94PORT\").alias(\"port_of_entry\"),\n",
    "                    col(\"ARRDATE\").alias(\"arrival_date\").cast(\"int\"),\n",
    "                    col(\"I94MODE\").alias(\"mode_of_transportation\").cast(\"int\"),\n",
    "                    col(\"I94ADDR\").alias(\"state_of_arrival_code\"),\n",
    "                    col(\"DEPDATE\").alias(\"departure_date\").cast(\"int\"),\n",
    "                    col(\"I94VISA\").alias(\"visa_type\").cast(\"int\"),) \\\n",
    ".withColumn(\"mode_of_transportation\", transportation_mapping_expr[col(\"mode_of_transportation\")]) \\\n",
    ".withColumn(\"state_of_arrival\", state_mapping_expr[col(\"state_of_arrival_code\")]) \\\n",
    ".withColumn(\"sas_date\", to_date(lit(\"01/01/1960\"), \"MM/dd/yyyy\")) \\\n",
    ".withColumn(\"arrival_date\", expr(\"date_add(sas_date, arrival_date)\")) \\\n",
    ".withColumn(\"departure_date\", expr(\"date_add(sas_date, departure_date)\")) \\\n",
    ".withColumn(\"visa_type\", visa_mapping_expr[col(\"visa_type\")]) \\\n",
    ".drop('sas_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_fact.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_dim = immigration_staging.join(country_code_mapping_staging, \n",
    "                         immigration_staging[\"I94CIT\"] == country_code_mapping_staging[\"code\"], \n",
    "                         \"left\").withColumnRenamed(\"country\", \"country_of_origin\").drop('code')\n",
    "person_dim = person_dim.join(country_code_mapping_staging, \n",
    "                         person_dim[\"I94RES\"] == country_code_mapping_staging[\"code\"], \n",
    "                         \"left\").withColumnRenamed(\"country\", \"country_of_residence\").drop('code')\n",
    "person_dim = person_dim.select(col(\"cicid\").alias(\"id\").cast(\"int\"),\n",
    "                                            col(\"GENDER\").alias(\"gender\"),\n",
    "                                            col(\"I94BIR\").alias(\"age\").cast(\"int\"),\n",
    "                                            col(\"country_of_origin\").alias(\"country_of_origin\"),\n",
    "                                            col(\"country_of_residence\").alias(\"country_of_residence\"),\n",
    "                                            col(\"BIRYEAR\").alias(\"year_of_birth\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_dim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographics dimension\n",
    "demographics_dim = demographics_staging.withColumn(\"id\", monotonically_increasing_id()) \\\n",
    "                    .select(col(\"id\"),\n",
    "                            col(\"State\").alias(\"state\"),\n",
    "                            col(\"Median Age\").alias(\"median_age\"),\n",
    "                            col(\"Female Population\").alias(\"female_population\"),\n",
    "                            col(\"Number of Veterans\").alias(\"number_of_veterans\"),\n",
    "                            col(\"Foreign-born\").alias(\"foreign_born\"),\n",
    "                            col(\"Average Household Size\").alias(\"average_household_size\"),\n",
    "                            col(\"State Code\").alias(\"state_code\"),\n",
    "                            col(\"Race\").alias(\"race\"),\n",
    "                            col(\"Count\").alias(\"count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_dim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_dim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dim = weather_staging.withColumn(\"id\", monotonically_increasing_id()) \\\n",
    "                              .select(col(\"id\"),\n",
    "                                     col(\"dt\").alias(\"date\"),\n",
    "                                     col(\"AverageTemperature\").alias(\"average_temperature\"),\n",
    "                                     col(\"AverageTemperatureUncertainty\").alias(\"average_temperature_uncertainty\"),\n",
    "                                     col(\"State\").alias(\"state\"),\n",
    "                                     col(\"Country\").alias(\"country\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_dim = airport_staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_staging.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# airport_staging.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
